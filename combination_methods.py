# -*- coding: utf-8 -*-
"""
COMBINATION METHODS module

This module contains definitions of forecast combinations methods, which are
used to produce the combine forecasts.

The first input should be a DataFrame with the training data of the following
form: the first column contains the realized values, the other columns contain
individual forecasts, the row index corresponds to time axis. The second input
should be a DataFrame with the test data (i.e. all columns contain individual
forecasts). The remaining input should be parameters relevant for each
respective method.

The output is a DataFrame with one column containing combined forecasts
(predictions) of length corresponding to the length of the supplied test
DataFrame.

"""

import pandas as pd
import numpy as np
from sklearn import linear_model
from sklearn.decomposition import PCA
from sklearn.neural_network import MLPRegressor
import time

"""
SIMPLE METHODS

"""


def Equal_Weights(df_test):
    """
    For this combining method, no training set is actually necessary. The
    predictions can be generated by averaging the individual forecasts supplied
    for testing.

    """

    df_pred = pd.DataFrame({"Equal Weights": np.mean(df_test, axis=1)})

    return df_pred


def Bates_Granger_1(df_train, df_test, nu):
    """
    This method combines the individual forecasts linearly, using the length
    of the training window nu.

    Firstly, the vector of squared forecast errors is calculated for each
    of the individual forecasts. Secondly, the vector of weights for prediction
    is calculated based on the error vectors. Lastly, the weights are used to
    combine forecasts and produce prediction for testing dataset.

    """

    # number of periods
    T = df_train.shape[0]

    if nu > T:
        raise ValueError('Parameter nu must be <= length of training sample')

    # forecast errors
    errors = df_train.iloc[:, 1:].subtract(df_train.iloc[:, 0], axis=0)
    sq_errors = errors**2

    # combining weights
    nominator = 1 / sq_errors.iloc[sq_errors.shape[0]-nu:, :].sum(axis=0)
    denominator = nominator.sum()
    comb_w = nominator / denominator

    # predictions
    df_pred = pd.DataFrame({"Bates-Granger (1)": df_test.dot(comb_w)})

    return df_pred


def Bates_Granger_2(df_train, df_test, nu):
    """
    This method combines the individual forecasts linearly, using the length
    of the training window nu.

    Firstly, the vector of forecast errors is calculated for each
    of the individual forecasts. Secondly, the estimate of the covariance
    matrix sigma is calculated. Lastly, the weights are used to combine
    forecasts and produce prediction for testing dataset. The weights lying
    out of the [0,1] interval are replaced by the appropriate end points.

    In order for covariance matrix sigma to be invertible, it is necesarry,
    that the parameter nu is greater or equal to the number of forecast to
    be combined.

    """
    # number of individual forecasts and number of periods
    K = df_test.shape[1]
    T = df_train.shape[0]

    if nu > T:
        raise ValueError('Parameter nu must be <= length of training sample')

    # check whether there is enough observations, so sigma is invertible
    if nu < K:
        raise ValueError('Parameter nu must be >= no. of individual forecasts')

    # forecast errors
    errors = df_train.iloc[:, 1:].subtract(df_train.iloc[:, 0], axis=0)

    # initialize the covariance matrix sigma
    sigma = np.full((K, K), fill_value=0, dtype=float)

    # fill the covariance matrix sigma
    for i in range(K):

        for j in range(K):

            sigma[i, j] = np.dot(errors.iloc[errors.shape[0]-nu:, i],
                                 errors.iloc[errors.shape[0]-nu:, j]) / nu

    # combining weights
    nominator = np.linalg.solve(sigma, np.full(K, fill_value=1))
    denominator = np.dot(np.full(K, fill_value=1), nominator)
    comb_w = nominator / denominator

    # censoring the combining weights
    for i in range(K):
        if comb_w[i] < 0:
            comb_w[i] = 0
        if comb_w[i] > 1:
            comb_w[i] = 1

    # rescale the weights so that their sum equals 1
    comb_w = comb_w/comb_w.sum()

    # predictions
    df_pred = pd.DataFrame({"Bates-Granger (2)": df_test.dot(comb_w)})

    return df_pred


def Bates_Granger_3(df_train, df_test, nu, alpha):
    """

    This method convexely combines the weights obtained from Bates-Granger
    method (1) and the preceeding weights, assuming that omega_{i,1} = 1 / K
    for all i. The combinations is determined by the parameter alpha.

    """

    # number of individual forecasts and number of periods
    K = df_test.shape[1]
    T = df_train.shape[0]

    if nu > T:
        raise ValueError('Parameter nu must be <= length of training sample')

    if nu < K:
        raise ValueError('Parameter nu must be >= no. of individual forecasts')

    # matrix of combination weights (t = 1,...,T, T+1), T+1 is for the final c.
    mat_comb_w = np.full((T+1, K), fill_value=0, dtype=float)

    # initialize with equal weights
    mat_comb_w[:nu, :] = np.full(K, fill_value=1 / K, dtype=float)

    # roll over the training period and calculate the combining weights
    for i in range(nu, T+1):

        # compute the weights using Bates-Granger method 1
        # forecast errors
        errors = df_train.iloc[:i, 1:].subtract(df_train.iloc[:i, 0], axis=0)
        sq_errors = errors**2

        # combining weights
        nominator = 1 / sq_errors.iloc[sq_errors.shape[0]-nu:, :].sum(axis=0)
        denominator = nominator.sum()
        method_1_comb_w = nominator / denominator

        # calculate and store the combined combining weights
        mat_comb_w[i, :] = alpha*mat_comb_w[i-1, :] + (1-alpha)*method_1_comb_w

    # final combining weights (weights for period T+1 = index T)
    comb_w = mat_comb_w[T, :]

    # predictions
    df_pred = pd.DataFrame({"Bates-Granger (3)": df_test.dot(comb_w)})

    return df_pred


def Bates_Granger_4(df_train, df_test, W):
    """
    This method combines the individual forecasts linearly, and uses the weight
    parameter W to assign more weight to recent errors.

    Firstly, the vector of squared forecast errors is calculated for each
    of the individual forecasts. Secondly, the vector of weights for prediction
    is calculated based on the error vectors. Lastly, the weights are used to
    combine forecasts and produce prediction for testing dataset.

    """

    # number of individual forecasts
    T = df_train.shape[0]

    # forecast errors
    errors = df_train.iloc[:, 1:].subtract(df_train.iloc[:, 0], axis=0)
    sq_errors = errors**2

    # exponential error weights
    error_w = np.full(T, fill_value=W, dtype=float)**(np.arange(T)+1)

    # combining weights
    nominator = 1 / np.dot(error_w, sq_errors)
    denominator = nominator.sum()
    comb_w = nominator / denominator

    # predictions
    df_pred = pd.DataFrame({"Bates-Granger (4)": df_test.dot(comb_w)})

    return df_pred


def Bates_Granger_5(df_train, df_test, W):
    """
    This method resembles the second method, except that it uses the weight
    parameter W to assign more weight to recent errors.

    Firstly, the vector of forecast errors is calculated for each
    of the individual forecasts. Secondly, the estimate of the covariance
    matrix sigma is calculated. Lastly, the weights are used to combine
    forecasts and produce prediction for testing dataset. The weights lying
    out of the [0,1] interval are replaced by the appropriate end points.

    In order for covariance matrix sigma to be invertible, it is necesarry,
    that the length of the training sample is greater or equal to the number
    of forecast to be combined.

    """
    # number of individual forecasts and number of periods
    K = df_test.shape[1]
    T = df_train.shape[0]

    # check whether there is enough observations, so sigma is invertible
    if K > T:
        raise ValueError('No. forecasts must be <= length of training sample')

    # forecast errors
    errors = df_train.iloc[:, 1:].subtract(df_train.iloc[:, 0], axis=0)

    # initialize the covariance matrix sigma
    sigma = np.full((K, K), fill_value=0, dtype=float)

    # exponential error weights
    error_w = np.full(T, fill_value=W, dtype=float)**(np.arange(T)+1)
    error_w_sum = error_w.sum()

    # fill the covariance matrix sigma
    for i in range(K):

        for j in range(K):

            # elements in sigma matrix are weighted with W
            sigma[i, j] = np.dot(error_w*errors.iloc[:, i],
                                 errors.iloc[:, j]) / error_w_sum

    # combining weights
    nominator = np.linalg.solve(sigma, np.full(K, fill_value=1))
    denominator = np.dot(np.full(K, fill_value=1), nominator)
    comb_w = nominator / denominator

    # censoring the combining weights
    for i in range(K):
        if comb_w[i] < 0:
            comb_w[i] = 0
        if comb_w[i] > 1:
            comb_w[i] = 1

    # rescale the weights so that their sum equals 1
    comb_w = comb_w/comb_w.sum()

    # predictions
    df_pred = pd.DataFrame({"Bates-Granger (5)": df_test.dot(comb_w)})

    return df_pred


def Granger_Ramanathan_1(df_train, df_test):
    """
    This method combines the individual forecasts linearly, and uses the weight
    parameters estimated using OLS.

    In the first method, the weights are estimated using unconstrained
    regression without an intercept. The weights are then used to combine
    forecasts and produce predictions for testing dataset.

    """
    # define y, F
    y = df_train.iloc[:, 0]
    F = df_train.iloc[:, 1:]

    # create linear regression object
    lin_reg = linear_model.LinearRegression(fit_intercept=False)

    # fit the model
    lin_reg.fit(F, y)

    # compute the combining weights
    beta_hat = lin_reg.coef_

    # predictions
    df_pred = pd.DataFrame({"Granger-Ramanathan (1)": df_test.dot(beta_hat)})

    return df_pred


def Granger_Ramanathan_2(df_train, df_test):
    """
    This method combines the individual forecasts linearly, and uses the weight
    parameters estimated using OLS.

    In the second method, the weights are estimated using contrained regression
    (sum of coefficients equals one) without an intercept. As described in the
    thesis I use the other, computationally equivalent way, of estimating the
    weights. Firstly, I compute the y_star and F_star by subtracting the
    last individual forecast. Secondly I estimate the beta_star and then I
    compute the last weight as 1-beta_star. The weights are then used to
    combine forecasts and produce predictions for testing dataset.

    """

    # number of individual forecasts
    K = df_test.shape[1]

    # define y_star, F_star
    y_star = df_train.iloc[:, 0] - df_train.iloc[:, K]
    F_star = df_train.iloc[:, 1:K].subtract(df_train.iloc[:, K], axis=0)

    # create linear regression object
    lin_reg = linear_model.LinearRegression(fit_intercept=False)

    # fit the model
    lin_reg.fit(F_star, y_star)

    # compute the combining weights
    beta_star_hat = lin_reg.coef_
    beta_K = 1 - beta_star_hat.sum()
    beta_hat = np.append(beta_star_hat, beta_K)

    # predictions
    df_pred = pd.DataFrame({"Granger-Ramanathan (2)": df_test.dot(beta_hat)})

    return df_pred


def Granger_Ramanathan_3(df_train, df_test):
    """
    This method combines the individual forecasts linearly, and uses the weight
    parameters estimated using OLS.

    In the third method, the weights are estimated using unconstrained
    regression with an intercept. The weights and intercept are then used to
    combine forecasts and produce predictions for testing dataset.

    """

    # define y, F
    y = df_train.iloc[:, 0]
    F = df_train.iloc[:, 1:]

    # create linear regression object
    lin_reg = linear_model.LinearRegression(fit_intercept=True)

    # fit the model
    lin_reg.fit(F, y)

    # store the estimated beta and alpha
    alpha_hat = lin_reg.intercept_
    beta_hat = lin_reg.coef_

    # predictions
    df_pred = pd.DataFrame({"Granger-Ramanathan (3)":
                            alpha_hat + df_test.dot(beta_hat)})

    return df_pred


def AFTER(df_train, df_test, lambd):
    """
    This method combines the individual forecasts linearly, using the tuning
    parameter alpha.

    Firstly, the vector of squared forecast errors is calculated for each
    of the individual forecasts. Secondly, the vector of weights for prediction
    is calculated based on the error vectors. Lastly, the weights are used to
    combine forecasts and produce prediction for testing dataset.

    """

    # forecast errors
    errors = df_train.iloc[:, 1:].subtract(df_train.iloc[:, 0], axis=0)
    sq_errors = errors**2

    # combining weights
    nominator = np.exp((-lambd) * sq_errors.sum(axis=0))
    denominator = nominator.sum()
    comb_w = nominator / denominator

    # predictions
    df_pred = pd.DataFrame({"AFTER": df_test.dot(comb_w)})

    return df_pred


def Median_Forecast(df_test):
    """
    For this combining method, no training set is actually necessary. The
    predictions can obtained by as a median from the forecasts supplied
    for testing.

    """

    df_pred = pd.DataFrame(
                {"Median Forecast": np.median(df_test, axis=1)},
                index=df_test.index
                )

    return df_pred


def Trimmed_Mean_Forecast(df_test, alpha):
    """
    For this combining method, no training set is actually necessary. The
    predictions can obtained by as an alpha-trimmed mean from the forecasts
    supplied for testing.

    """

    # number of individual forecasts
    K = df_test.shape[1]

    # number values to be removed
    r = np.floor(alpha*K).astype(int)

    # trimmed testing set
    df_test_trim = np.sort(df_test)[:, r:(K-r)]

    # predictions
    df_pred = pd.DataFrame(
                {"Trimmed Mean Forecast": np.mean(df_test_trim, axis=1)},
                index=df_test.index
                )

    return df_pred


def PEW(df_train, df_test):
    """
    The projection on the equall weights: regress the variable to be forecast
    on the average of individual forecasts (with intercept).

    """

    # define y, f_bar
    y = df_train.iloc[:, 0]
    f_bar = np.mean(df_train.iloc[:, 1:], axis=1).values.reshape(-1, 1)

    # create linear regression object
    lin_reg = linear_model.LinearRegression(fit_intercept=True)

    # fit the model
    lin_reg.fit(f_bar, y)

    # store the estimated beta and alpha
    alpha_hat = lin_reg.intercept_
    beta_hat = lin_reg.coef_

    # predictions
    df_pred = pd.DataFrame({"PEW":
                            alpha_hat + beta_hat*np.mean(df_test, axis=1)})

    return df_pred


"""
FACTOR ANALYTIC METHODS

"""


def Principal_Component_Forecast(df_train, df_test, prcomp):
    """
    The principal component (factor analytic) combination method. The number
    of used components depends on the parameter "prcomp" = {single, AIC, BIC}

    """

    # number of periods
    T = df_train.shape[0]

    # compute second moment matrix of forecasts
    sec_mom_mat = np.dot(np.transpose(df_train.iloc[:, 1:]),
                         df_train.iloc[:, 1:])

    if (prcomp == "single"):

        # estimate the common factor mu using principal components
        pca = PCA(n_components=1).fit(sec_mom_mat)
        lambda_hat = np.transpose(pca.components_)
        mu_hat = np.dot(df_train.iloc[:, 1:], lambda_hat)

        # define y
        y = df_train.iloc[:, 0]

        # create linear regression object
        lin_reg = linear_model.LinearRegression(fit_intercept=False)

        # fit the model
        lin_reg.fit(mu_hat, y)

        # store the estimated beta
        beta_hat = lin_reg.coef_

        # predictions
        df_pred = pd.DataFrame(
                {"Principal Component Forecast":
                    np.dot(np.dot(df_test, lambda_hat), beta_hat)},
                index=df_test.index)

    if (prcomp == "AIC"):

        # initialize final beta coefficients and AIC
        final_beta_hat = np.nan
        final_lambda_hat = np.nan
        final_AIC = np.inf

        for i in range(4):
            # number of principal components, +1 for python
            prcomp_no = i + 1

            # estimate the common factor mu using principal components
            pca = PCA(n_components=prcomp_no).fit(sec_mom_mat)
            lambda_hat = np.transpose(pca.components_)
            mu_hat = np.dot(df_train.iloc[:, 1:], lambda_hat)

            # define y
            y = df_train.iloc[:, 0]

            # create linear regression object
            lin_reg = linear_model.LinearRegression(fit_intercept=False)

            # fit the model
            lin_reg.fit(mu_hat, y)

            # store the estimated beta
            beta_hat = lin_reg.coef_

            # AIC calculation
            res = y - np.dot(mu_hat, beta_hat)
            res_sq = res**2
            sigma_sq = sum(res_sq)/T
            P = prcomp_no + 1  # total number of parameters, including sigma
            AIC = T*np.log(sigma_sq) + 2*P

            # change final beta and AIC, if there is improvement in AIC
            if AIC < final_AIC:
                final_AIC = AIC
                final_beta_hat = beta_hat
                final_lambda_hat = lambda_hat

        # predictions
        df_pred = pd.DataFrame(
                {"Principal Component Forecast (AIC)":
                    np.dot(np.dot(df_test, final_lambda_hat), final_beta_hat)},
                index=df_test.index)

    if (prcomp == "BIC"):

        # initialize final beta coefficients, eigenvectors and BIC
        final_beta_hat = np.nan
        final_lambda_hat = np.nan
        final_BIC = np.inf

        for i in range(4):
            # number of principal components, +1 for python
            prcomp_no = i + 1

            # estimate the common factor mu using principal components
            pca = PCA(n_components=prcomp_no).fit(sec_mom_mat)
            lambda_hat = np.transpose(pca.components_)
            mu_hat = np.dot(df_train.iloc[:, 1:], lambda_hat)

            # define y
            y = df_train.iloc[:, 0]

            # create linear regression object
            lin_reg = linear_model.LinearRegression(fit_intercept=False)

            # fit the model
            lin_reg.fit(mu_hat, y)

            # store the estimated beta
            beta_hat = lin_reg.coef_

            # BIC calculation
            res = y - np.dot(mu_hat, beta_hat)
            res_sq = res**2
            sigma_sq = sum(res_sq)/T
            P = prcomp_no + 1  # total number of parameters, including sigma
            BIC = T*np.log(sigma_sq) + P*np.log(T)

            # change the final coefficients, if there is improvement in BIC
            if BIC < final_BIC:
                final_BIC = BIC
                final_beta_hat = beta_hat
                final_lambda_hat = lambda_hat

        # predictions
        df_pred = pd.DataFrame(
                {"Principal Component Forecast (BIC)":
                    np.dot(np.dot(df_test, final_lambda_hat), final_beta_hat)},
                index=df_test.index)

    return df_pred


"""
SHRINKING METHODS

"""


def Empirical_Bayes_Estimator(df_train, df_test):
    """
    The empirical bayes estimator of combining weights. Bayesian linear
    regression model with the prior specified in Diebold & Pauli (1990).

    """

    # number of individual forecasts and number of periods
    K = df_test.shape[1]
    T = df_train.shape[0]

    # define the prior weights (simple average, with intercept equal zero)
    beta_0 = np.append(np.array(0), np.full(K, fill_value=1/K, dtype=float))

    # design matrix (intercept + individual forecasts)
    F = df_train.iloc[:, 1:].values
    F = np.insert(F, 0, 1, axis=1)

    # define y (observed values)
    y = df_train.iloc[:, 0].values

    # OLS weights
    beta_hat = np.dot(np.linalg.inv(np.dot(np.transpose(F), F)),
                      np.dot(np.transpose(F), y))

    # sigma
    sigma_hat_sq = np.dot(
            np.transpose(y - np.dot(F, beta_hat)),
            y - np.dot(F, beta_hat)
            ) / T

    # tau
    num = np.dot(np.transpose(beta_hat - beta_0), beta_hat - beta_0)
    denum = np.trace(np.linalg.inv(np.dot(np.transpose(F), F)))
    tau_hat_sq = num / denum - sigma_hat_sq

    # combining weights
    shrinkage = 1 - sigma_hat_sq / (sigma_hat_sq + tau_hat_sq)
    beta_1_hat = beta_0 + shrinkage*(beta_hat - beta_0)

    # predictions
    df_pred = pd.DataFrame(
            {"Empirical Bayes Estimator":
                beta_1_hat[0] + np.dot(df_test, beta_1_hat[1:])},
            index=df_test.index)

    return df_pred


def Kappa_Shrinkage(df_train, df_test, kappa):
    """
    Shrinkage of the combining weights from OLS without intercept towards
    the equal weights. The amount of shrinkage is driven by parameter kappa.

    """

    # number of individual forecasts and number of periods
    K = df_test.shape[1]
    T = df_train.shape[0]

    # define the prior weights (simple average, with intercept equal zero)
    beta_0 = np.full(K, fill_value=1/K, dtype=float)

    # design matrix (intercept + individual forecasts)
    F = df_train.iloc[:, 1:].values

    # define y (observed values)
    y = df_train.iloc[:, 0].values

    # OLS weights (without an intercept)
    beta_hat = np.dot(np.linalg.inv(np.dot(np.transpose(F), F)),
                      np.dot(np.transpose(F), y))

    # shrinkage weight
    lambd = max([0, 1 - kappa * (K / (T - 1 - K))])

    # combining weights
    comb_w = lambd*beta_hat + (1-lambd)*beta_0

    # predictions
    df_pred = pd.DataFrame({"Kappa-Shrinkage": df_test.dot(comb_w)})

    return df_pred


def LASSO_coef(df_train, lambda_1):
    """
    Refer down to 2-step Egalitarian LASSO.

    """

    # define y, X
    y = df_train.iloc[:, 0]
    X = df_train.iloc[:, 1:]

    # estimate LASSO
    lasso = linear_model.Lasso(alpha=lambda_1,
                               fit_intercept=False,
                               max_iter=100000).fit(X, y)

    # which forecasters to keep
    return np.append(True, lasso.coef_ != 0)  # 1s True for y


def Egalitarian_LASSO(df_train, lambda_2):
    """
    Refer down to 2-step Egalitarian LASSO.

    """

    # definitions
    y = df_train.iloc[:, 0]
    X = df_train.iloc[:, 1:]
    f_bar = np.mean(X, axis=1)
    K = X.shape[1]

    # estimate Egalitarian LASSO via the transformation to LASSO
    egal_lasso = linear_model.Lasso(alpha=lambda_2,
                                    fit_intercept=False,
                                    max_iter=100000)

    beta = egal_lasso.fit(X, y-f_bar).coef_ + 1/K

    return beta


def Two_Step_Egalitarian_LASSO(df_train, df_test, k_cv):
    """
    The 2-step Egalitarian LASSO procedures. Firstly, we need to define
    the LASSO function as a first step and Egalitarian LASSO function as
    a second step. Then the grid of 200 potential values of the shrinkage
    parameters and finally. Then the optimal pair of lambdas is searched for
    using leave-one-out cross validation and finally, the combining weights
    are computed using both steps with the optimal pair of lambdas.

    In order to reduce computation intensity, only k-fold cross-validation is
    used.

    """

    # grid for the pair of shrinkage parameters
    lambda_grid = np.exp(np.linspace(-6, 2, num=20))

    lambda_mat = pd.DataFrame(
            0,
            dtype=float,
            index=lambda_grid,
            columns=lambda_grid)

    # length of cross-validation subset
    k_len = int(df_train.shape[0] / k_cv)

    # for t in range(T):
    for k in range(k_cv):

        df_train_rd = df_train.loc[
                ~df_train.index.isin(df_train.index[k:(k+k_len)]), :]

        # matrix of selected forecasters (dropping duplicate results)
        sel_forecast_mat = pd.DataFrame(np.nan,
                                        dtype=bool,
                                        index=lambda_grid,
                                        columns=df_train.columns)

        for i in range(lambda_grid.size):
            sel_forecast_mat.loc[lambda_grid[i], :] = LASSO_coef(
                                                        df_train_rd,
                                                        lambda_grid[i]
                                                        )

        # sel_forecast_mat = sel_forecast_mat.drop_duplicates()

        # atleast one selected forecaster
        # sel_forecast_mat = sel_forecast_mat.loc[
        #         np.sum(sel_forecast_mat, axis=1) > 1, :]

        # target
        real_val = df_train.iloc[k:(k+k_len), 0]

        # optimize lambda 2
        for i in range(sel_forecast_mat.shape[0]):

            for j in range(lambda_grid.size):
                # obtain coefficients
                if np.sum(sel_forecast_mat, axis=1).iloc[i] > 1:
                    beta = Egalitarian_LASSO(
                            df_train_rd.loc[:, sel_forecast_mat.iloc[i, :]],
                            lambda_grid[j]
                            )

                    # update MSE
                    cv_test_df = df_train.loc[
                                    df_train.index[k:(k+k_len)],
                                    sel_forecast_mat.iloc[i, :]
                                    ]
                    prediction = np.dot(cv_test_df.iloc[:, 1:], beta)

                    # reshape necesarry if multiplying by 1 forecaster
                    if np.sum(sel_forecast_mat, axis=1).iloc[i] == 2:

                        lambda_mat.iloc[i, j] += sum(
                            (real_val.values.reshape(-1, 1) - prediction)**2
                            ) / k_len
                    else:
                        lambda_mat.iloc[i, j] += sum(
                            (real_val - prediction)**2
                            ) / k_len
                else:
                    lambda_mat.iloc[i, j] = np.inf

        # optimal lambda (multiple minima may occur when shrunk to equal w.)
        lamda_mat_tf = (lambda_mat == np.min(lambda_mat.values))
        lamda_star_1 = lamda_mat_tf.sum(axis=1)[
                lamda_mat_tf.sum(axis=1) > 0].index.values[0]
        lamda_star_2 = lamda_mat_tf.sum(axis=0)[
                lamda_mat_tf.sum(axis=0) > 0].index.values[0]

        # estimation with optimal lambda
        df_coef = LASSO_coef(df_train, lamda_star_1)
        beta = Egalitarian_LASSO(df_train.loc[:, df_coef], lamda_star_2)

        # predictions
        df_test_rd = df_test.loc[:, df_coef[1:]]

        if beta.size == 1:
            final_pred = df_test_rd * beta
        else:
            final_pred = df_test_rd.dot(beta)

        df_pred = pd.DataFrame({"Two-Step Egalitarian LASSO": final_pred},
                               index=df_test_rd.index)

    return df_pred


def BMA_Marginal_Likelihood(df_train, df_test, iterations, burnin, p_1):
    """
    The combination of forecasts using the bayesian model averaging. In this
    case, the usual marginal likelihoods are used to compute the posterior
    model probabilities. The posterior probabilities are then used as weights
    to combine the models into the single forecast.

    For searching the model space is used Reversible Jump Markov Chain
    Monte Carlo (RJ-MCMC). First x of iterations are discarded as burnin.
    The resulting combination from this method is computed conditinally
    on the set of models visited by the chain.

    """

    # number of individual forecasts and number of periods
    K = df_test.shape[1]
    T = df_train.shape[0]

    # g-prior parameter
    c = float(K**2)

    # separate real value and individual forecasts
    y = df_train.iloc[:, 0]
    F = df_train.iloc[:, 1:]

    # initializations
    models = np.full((iterations, K), fill_value=False, dtype=bool)
    theta_hats = []
    marginal_lik = np.full(iterations, fill_value=np.nan, dtype=float)

    # RJ-MCMC
    # initial state = null model
    # marginal likelihood for the initial state
    y_bar = np.mean(y)
    S_right = sum((y - y_bar)**2)/(c+1)

    Z = np.insert(F.iloc[:, models[0, :]].values, 0, 1, axis=1)
    theta_hat = np.dot(
            np.linalg.inv(np.dot(np.transpose(Z), Z)),
            np.dot(np.transpose(Z), y)
            )
    SSR = sum((y - np.dot(Z, theta_hat))**2)
    S_left = (c/(c+1))*SSR
    S = S_left + S_right
    k = sum(models[0, :])
    marginal_lik[0] = ((c+1)**(-k))*(S**(-((T-1)/2)))
    theta_hats += [theta_hat]

    # initialization of the proposal model
    m_star = np.full(K, fill_value=True, dtype=bool)

    # main iteration loop
    for i in range(0, iterations-1):

        # decide which move to perform
        epsilon_move = np.random.random()

        # move 1
        if epsilon_move <= p_1 or sum(models[0]) == 0:

            # drawn one of the forecasts from the model
            drawn_f_index = np.random.randint(0, K)

            # if it is in the model, drop it, if not, add it
            np.copyto(m_star, models[i, :])
            m_star[drawn_f_index] = ~models[i, drawn_f_index]

            # if there are too many variables (d.f. = 0), the change is revert
            if sum(m_star) > (T-2):
                m_star[drawn_f_index] = models[i, drawn_f_index]

        # move 2
        else:

            # drawn one of the forecasts from the model and one outside
            index_in = np.arange(K)[models[i, :]]
            index_out = np.arange(K)[~models[i, :]]
            drawn_f_index_in = np.random.choice(index_in)
            drawn_f_index_out = np.random.choice(index_out)

            # swap these two variables
            np.copyto(m_star, models[i, :])
            m_star[drawn_f_index_in] = ~m_star[drawn_f_index_in]
            m_star[drawn_f_index_out] = ~m_star[drawn_f_index_out]

        # calculate the probability of acceptance
        Z = np.insert(F.iloc[:, m_star].values, 0, 1, axis=1)
        theta_hat = np.dot(
                np.linalg.inv(np.dot(np.transpose(Z), Z)),
                np.dot(np.transpose(Z), y)
                )
        SSR = sum((y - np.dot(Z, theta_hat))**2)
        S_left = (c/(c+1))*SSR
        S = S_left + S_right
        k = sum(m_star)
        marginal_lik_m_star = ((c+1)**(-k))*(S**(-((T-1)/2)))

        alpha = min(1, marginal_lik_m_star/marginal_lik[i])

        # if accepted, move to m_star, else retain the same model
        epsilon_accept = np.random.random()

        if epsilon_accept <= alpha:
            np.copyto(models[i+1, :], m_star)
            marginal_lik[i+1] = marginal_lik_m_star
            theta_hats += [theta_hat]
        else:
            np.copyto(models[i+1, :], models[i, :])
            marginal_lik[i+1] = marginal_lik[i]
            theta_hats += [theta_hats[-1]]

    # discard the burnin draws
    models = models[burnin:, :]
    marginal_lik = marginal_lik[burnin:]
    theta_hats = theta_hats[burnin:]

    # unique models visited by the chain
    models_uni, index_uni = np.unique(models, axis=0, return_index=True)
    marginal_lik_uni = marginal_lik[index_uni]
    # theta_hats_uni = [theta_hats[i] for i in index_uni]
    theta_hats_uni = list(np.array(theta_hats)[index_uni])

    # model posterior probabilities
    marginal_lik_sum = sum(marginal_lik_uni)
    posterior_prob = marginal_lik_uni / marginal_lik_sum

    # generate prediction for testing set
    model_fcts = np.full(
            (df_test.shape[0], len(theta_hats_uni)),
            np.nan,
            dtype=float
            )

    # forecast from different models
    for j in range(model_fcts.shape[1]):

        model_fcts[:, j] = np.dot(
                np.insert(df_test.iloc[:, models_uni[j]].values, 0, 1, axis=1),
                theta_hats_uni[j]
                )

    pred = np.dot(model_fcts, posterior_prob)

    df_pred = pd.DataFrame(
            {"BMA (Marginal Likelihood)":  pred},
            index=df_test.index)

    return df_pred


def BMA_Predictive_Likelihood(df_train, df_test, iterations, burnin, p_1,
                              l_share):
    """
    The combination of forecasts using the bayesian model averaging. In this
    case, the usual predictive densities are used to compute the posterior
    model probabilities. The posterior probabilities are then used as weights
    to combine the models into the single forecast.

    For searching the model space is used Reversible Jump Markov Chain
    Monte Carlo (RJ-MCMC). First x of iterations are discarded as burnin.
    The resulting combination from this method is computed conditinally
    on the set of models visited by the chain.

    """

    # number of individual forecasts and number of periods
    K = df_test.shape[1]
    T = df_train.shape[0]

    # g-prior parameter
    c = float(K**3)

    # separate real value and individual forecasts
    y = df_train.iloc[:, 0]
    F = df_train.iloc[:, 1:]

    # separate training and the hold-out sample
    m_share = 1 - l_share
    y_star = y.iloc[:int(np.round(m_share*T))]
    y_tilde = y.iloc[int(np.round(m_share*T)):]
    F_star = F.iloc[:int(np.round(m_share*T)), :]
    F_tilde = F.iloc[int(np.round(m_share*T)):, :]

    # initializations
    models = np.full((iterations, K), fill_value=False, dtype=bool)
    theta_hats = []
    predictive_lik = np.full(iterations, fill_value=np.nan, dtype=float)

    # RJ-MCMC
    # initial state = null model
    # predictive likelihood for the initial state
    y_star_bar = np.mean(y_star)
    S_right = sum((y_star - y_star_bar)**2)/(c+1)
    Z_star = np.insert(F_star.iloc[:, models[0, :]].values, 0, 1, axis=1)
    Z_star_t = np.transpose(Z_star)
    Z_tilde = np.insert(F_tilde.iloc[:, models[0, :]].values, 0, 1, axis=1)
    Z_tilde_t = np.transpose(Z_tilde)
    theta_hat = np.dot(
            np.linalg.inv(np.dot(np.transpose(Z_star), Z_star)),
            np.dot(np.transpose(Z_star), y_star)
            )
    theta_hats += [theta_hat]
    gamma = (c/(c+1)) * theta_hat
    SSR = sum((y_star - np.dot(Z_star, theta_hat))**2)
    S_left = (c/(c+1))*SSR
    S = S_left + S_right
    lambda_star = ((c+1) / c) * np.dot(Z_star_t, Z_star)
    lambda_star_inv = np.linalg.inv(lambda_star)
    m_len = y_star.shape[0]
    l_len = y_tilde.shape[0]
    epsilon = y_tilde - np.dot(Z_tilde, gamma)
    epsilon_t = np.transpose(epsilon)
    mid = np.linalg.inv(
            np.eye(l_len) + np.dot(np.dot(Z_tilde, lambda_star_inv), Z_tilde_t)
            )
    term_1 = S**(m_len/2)
    term_2 = (np.linalg.det(lambda_star) / np.linalg.det(
            lambda_star + np.dot(Z_tilde_t, Z_tilde)))**(1/2)
    term_3 = (S + np.dot(np.dot(epsilon_t, mid), epsilon))**(-T/2)
    predictive_lik[0] = term_1 * term_2 * term_3

    # initialization of the proposal model
    m_star = np.full(K, fill_value=True, dtype=bool)

    # main iteration loop
    for i in range(0, iterations-1):

        # decide which move to perform
        epsilon_move = np.random.random()

        # move 1
        if epsilon_move <= p_1 or sum(models[0]) == 0:

            # drawn one of the forecasts from the model
            drawn_f_index = np.random.randint(0, K)

            # if it is in the model, drop it, if not, add it
            np.copyto(m_star, models[i, :])
            m_star[drawn_f_index] = ~models[i, drawn_f_index]

            # if there are too many variables (d.f. = 0), the change is revert
            if sum(m_star) > (m_len-2):
                m_star[drawn_f_index] = models[i, drawn_f_index]

        # move 2
        else:

            # drawn one of the forecasts from the model and one outside
            index_in = np.arange(K)[models[i, :]]
            index_out = np.arange(K)[~models[i, :]]
            drawn_f_index_in = np.random.choice(index_in)
            drawn_f_index_out = np.random.choice(index_out)

            # swap these two variables
            np.copyto(m_star, models[i, :])
            m_star[drawn_f_index_in] = ~m_star[drawn_f_index_in]
            m_star[drawn_f_index_out] = ~m_star[drawn_f_index_out]

        # calculate the probability of acceptance
        Z_star = np.insert(F_star.iloc[:, m_star].values, 0, 1, axis=1)
        Z_star_t = np.transpose(Z_star)
        Z_tilde = np.insert(F_tilde.iloc[:, m_star].values, 0, 1, axis=1)
        Z_tilde_t = np.transpose(Z_tilde)
        theta_hat = np.dot(
                np.linalg.inv(np.dot(np.transpose(Z_star), Z_star)),
                np.dot(np.transpose(Z_star), y_star)
                )
        gamma = (c/(c+1)) * theta_hat
        SSR = sum((y_star - np.dot(Z_star, theta_hat))**2)
        S_left = (c/(c+1))*SSR
        S = S_left + S_right
        lambda_star = ((c+1) / c) * np.dot(Z_star_t, Z_star)
        lambda_star_inv = np.linalg.inv(lambda_star)
        epsilon = y_tilde - np.dot(Z_tilde, gamma)
        epsilon_t = np.transpose(epsilon)
        mid = np.linalg.inv(
            np.eye(l_len) + np.dot(np.dot(Z_tilde, lambda_star_inv), Z_tilde_t)
            )
        term_1 = S**(m_len/2)
        term_2 = (np.linalg.det(lambda_star) / np.linalg.det(
            lambda_star + np.dot(Z_tilde_t, Z_tilde)))**(1/2)
        term_3 = (S + np.dot(np.dot(epsilon_t, mid), epsilon))**(-T/2)
        predictive_lik_m_star = term_1 * term_2 * term_3
        alpha = min(1, predictive_lik_m_star/predictive_lik[i])

        # if accepted, move to m_star, else retain the same model
        epsilon_accept = np.random.random()

        if epsilon_accept <= alpha:
            np.copyto(models[i+1, :], m_star)
            predictive_lik[i+1] = predictive_lik_m_star
            theta_hats += [theta_hat]
        else:
            np.copyto(models[i+1, :], models[i, :])
            predictive_lik[i+1] = predictive_lik[i]
            theta_hats += [theta_hats[-1]]

    # discard the burnin draws
    models = models[burnin:, :]
    predictive_lik = predictive_lik[burnin:]
    theta_hats = theta_hats[burnin:]

    # unique models visited by the chain
    models_uni, index_uni = np.unique(models, axis=0, return_index=True)
    predictive_lik_uni = predictive_lik[index_uni]
    # theta_hats_uni = [theta_hats[i] for i in index_uni]
    theta_hats_uni = list(np.array(theta_hats)[index_uni])

    # model posterior probabilities
    predictive_lik_sum = sum(predictive_lik_uni)
    posterior_prob = predictive_lik_uni / predictive_lik_sum

    # generate prediction for testing set
    model_fcts = np.full(
            (df_test.shape[0], len(theta_hats_uni)),
            np.nan,
            dtype=float
            )

    # forecast from different models
    for j in range(model_fcts.shape[1]):

        model_fcts[:, j] = np.dot(
                np.insert(df_test.iloc[:, models_uni[j]].values, 0, 1, axis=1),
                theta_hats_uni[j]
                )

    pred = np.dot(model_fcts, posterior_prob)

    df_pred = pd.DataFrame(
            {"BMA (Predictive Likelihood)":  pred},
            index=df_test.index)

    return df_pred


def ANN(df_train, df_test):
    """
    Artificial Neural Network forecast combination method. It uses a single
    hidden layer with up to three logistic nodes, possibly complemented with
    K linear nodes. The 10-fold cross-validation is used to determine the
    optimal number of nodes.

    """

    # number of individual forecasts and number of periods
    K = df_test.shape[1]
    T = df_train.shape[0]
    T_all = df_train.shape[0] + df_test.shape[0]

    # matrix of individual forecasts (including train and test sets)
    F = np.concatenate((df_train.iloc[:, 1:].values, df_test.values), axis=0)

    # matrix of standardized forecasts and the intercept
    y = df_train.iloc[:, 0]
    y_bar = np.mean(y)
    y_std = np.std(y, ddof=1)

    Z = np.concatenate(
            (np.full((T_all, 1), 1, dtype=float),
             (F - y_bar) / y_std),
            axis=1)

    # randomly draw 10 Gamma sets (elements from uniform (-1, 1))
    len_Gamma = 10
    Gamma = []

    for i in range(len_Gamma):
        Gamma += [
                (np.random.rand(K+1, 3) * 2) - 1
                ]

    # prepare design matrices for the estimation
    X_list = []

    # add intercepts
    for i in range(len_Gamma * 2 * 3 + 1):
        X_list += [np.full((T_all, 1), 1, dtype=float)]

    # add linear nodes to half of the ANNs + the first fully linear model
    for i in range(len_Gamma * 3 + 1):

        X_list[i] = np.concatenate((X_list[i], F), axis=1)

    # add logistic nodes to all models (2 halves) except the first one
    for h in range(2):

        for i in range(len_Gamma):

            for p in range(1, 4):

                logistic_nodes = 1 / (1 + np.exp(-np.dot(Z, Gamma[i][:, :p])))

                X_list[(h * len_Gamma + i) * 3 + p] = np.concatenate(
                        (X_list[(h * len_Gamma + i) * 3 + p], logistic_nodes),
                        axis=1)

    # search for the best model specification using the 10-fold CV
    cv_sq_errors = np.full(len(X_list), 0, dtype=float)

    for i in range(10):

        # train and test indices
        cv_y_train_index = np.full(T, True, dtype=bool)
        cv_y_train_index[int(i * T/10):int((i+1) * T/10)] = False

        cv_X_train_index = np.append(cv_y_train_index,
                                     np.full(T_all-T, False, dtype=bool))
        cv_X_test_index = np.append(~cv_y_train_index,
                                    np.full(T_all-T, False, dtype=bool))

        # variable to be forecast
        cv_y_train = y[cv_y_train_index]
        cv_y_test = y[~cv_y_train_index]

        # design matrix for each model separately
        for j in range(len(X_list)):

            # set for training
            cv_X_train = X_list[j][cv_X_train_index, :]

            # set for testing
            cv_X_test = X_list[j][cv_X_test_index, :]

            # estimate the model
            est_par = np.dot(
                np.linalg.inv(np.dot(np.transpose(cv_X_train), cv_X_train)),
                np.dot(np.transpose(cv_X_train), cv_y_train)
                )

            # predict
            cv_y_pred = np.dot(cv_X_test, est_par)

            # save the squared errors
            cv_sq_errors[j] += np.sum((cv_y_test - cv_y_pred)**2)

    # turn the sum of squared errors to mean squared errors
    cv_MSE = cv_sq_errors/T

    # find the ANN specification with the minimal CV MSE
    best_spec_index = np.argmin(cv_MSE)

    # train the best ANN
    X_train = X_list[best_spec_index][:T, :]

    est_par = np.dot(
            np.linalg.inv(np.dot(np.transpose(X_train), X_train)),
            np.dot(np.transpose(X_train), y)
            )

    # final predictions
    X_test = X_list[best_spec_index][T:, :]

    pred = np.dot(X_test, est_par)

    df_pred = pd.DataFrame(
            {"ANN":  pred},
            index=df_test.index
            )

    return df_pred


def EP_NN(df_train, df_test, sigma, gen, n):
    """
    Evolving Artificial Neural Network forecast combination method.
    It uses a single hidden layer with three logistic nodes, complemented with
    K linear nodes. The paramaterers in the logistic nodes are optimized
    trough the stochastic numerical process. Median MSE EP-NN from 29
    independently estimated ones is taken as the final EP-NN.

    """

    # number of individual forecasts and number of periods
    K = df_test.shape[1]
    T = df_train.shape[0]
    T_all = df_train.shape[0] + df_test.shape[0]

    # matrix of individual forecasts
    F = df_train.iloc[:, 1:].values

    # matrix of standardized forecasts and the intercept
    y = df_train.iloc[:, 0]
    y_bar = np.mean(y)
    y_std = np.std(y, ddof=1)
    Y = y.values.reshape(T, 1)

    Z = np.concatenate(
            (np.full((T, 1), 1, dtype=float),
             (F - y_bar) / y_std),
            axis=1)

    # run the procedure 29 times to select the median EP-NN as the final one
    MSE_vec_star = np.full(29, 0, dtype=float)
    Gamma_star = np.full((29, K+1, 3), 0, dtype=float)

    for j in range(29):

        # the EP-NN procedure
        # step 1
        Gamma = (np.random.rand(n, K+1, 3) * 2) - 1

        # steps (2-5)
        for i in range(gen):

            # step 2
            logistic_nodes = 1 / (1 + np.exp(-np.matmul(Z, Gamma)))

            X = np.concatenate((np.full((T, 1), 1, dtype=float), F), axis=1)
            X = np.repeat(X[np.newaxis, :, :], n, axis=0)
            X = np.concatenate((X, logistic_nodes), axis=2)

            est_par = np.matmul(
                    np.linalg.inv(np.matmul(np.swapaxes(X, 1, 2), X)),
                    np.matmul(np.swapaxes(X, 1, 2), Y)
                    )

            Y_pred = np.matmul(X, est_par)
            MSE_vec = np.sum((Y - Y_pred)**2, axis=1)/T

            # step 3
            sort_ind = np.argsort(MSE_vec.flatten())
            Gamma = Gamma[sort_ind, :, :]

            # step 4
            eta = np.concatenate((
                    np.full((int(np.ceil(n/2)), K+1, 3), 0, dtype=float),
                    sigma * np.random.randn(int(np.floor(n/2)), K+1, 3)
                    ), axis=0)

            Gamma = Gamma + eta

        # sort also the last MSE_vec
        MSE_vec = MSE_vec[sort_ind]

        # save the Gamma star and its corresponding MSE
        MSE_vec_star[j] = MSE_vec[0]
        Gamma_star[j, :, :] = Gamma[0, :, :]

    # median MSE Gamma as the final Gamma
    median_ind = np.argsort(MSE_vec_star)[14]
    Gamma_fin = Gamma_star[median_ind, :, :]

    # train the final EP-NN, necessary to merge individual forecasts from
    # the train and test dataframes
    F_all = np.concatenate((df_train.iloc[:, 1:].values, df_test.values),
                           axis=0)
    Z_all = np.concatenate(
            (np.full((T_all, 1), 1, dtype=float),
             (F_all - y_bar) / y_std),
            axis=1)
    logistic_nodes_fin = 1 / (1 + np.exp(-np.matmul(Z_all, Gamma_fin)))
    F_fin = np.concatenate((np.full((T_all, 1), 1, dtype=float), F_all),
                           axis=1)
    X_fin = np.concatenate((F_fin, logistic_nodes_fin), axis=1)

    X_train = X_fin[:T, :]

    est_par_fin = np.dot(
            np.linalg.inv(np.dot(np.transpose(X_train), X_train)),
            np.dot(np.transpose(X_train), y)
            )

    # final predictions
    X_test = X_fin[T:, :]

    pred = np.dot(X_test, est_par_fin)

    df_pred = pd.DataFrame(
            {"EP-NN":  pred},
            index=df_test.index
            )

    return df_pred


def Bagging(df_train, df_test, B):
    """
    The Bagging  (Bootstrap aggregation) forecast combination method with
    a t-test pre-test for variable selection. B overlapping random blocks are
    randomly drawn from the training sample and for each is applied the
    pre-test and the model is trained after the variable selection. If no
    variable passes the pre-test, the prediction is zero. The final
    combination is the average of forecasts of these B models.

    """

    # number of individual forecasts and number of periods
    K = df_test.shape[1]
    T = df_train.shape[0]

    # matrix for saving predictions from bootstrapped models
    pred_mat = np.full((df_test.shape[0], B), np.nan, dtype=float)

    # length of the boostrap block
    m = int(T**(1/3))
    p = np.int(T/m)

    # pairwise bootstrap
    # for i in range(B):
    #
    #    # create the bootstrap sample from randomly drawn blocks
    #   boot_indices = np.random.randint(T, size=T)
    #    boot_X = df_train.iloc[boot_indices, 1:]
    #   boot_y = df_train.iloc[boot_indices, 0]
    #
    #    # pre-test
    #    # estimate OLS on the block with all individual forecasts
    #    boot_X_t = np.transpose(boot_X)
    #    XX = np.linalg.inv(np.dot(boot_X_t, boot_X))
    #    beta_hat = np.linalg.multi_dot([XX, boot_X_t, boot_y])
    #
    #    # residuals
    #    epsilon = boot_y - np.dot(boot_X, beta_hat)
    #    epsilon_sq = epsilon**2
    #
    #    # variances (heteroskedasticity robust)
    #    Sigma = np.diag(epsilon_sq)
    #    beta_var = np.diag(
    #            np.linalg.multi_dot([XX, boot_X_t, Sigma, boot_X, XX])
    #            )

    # moving block bootstrap
    for i in range(B):

        # create the bootstrap sample from randomly drawn blocks
        boot_X = np.full((p*m, K), np.nan, dtype=float)
        boot_y = np.full((p*m, 1), np.nan, dtype=float)

        for j in range(p):

            # draw the random block
            start_index = np.random.randint(T-m)
            end_index = start_index + m

            # fill the sample with another block
            boot_X[j*m:(j+1)*m, :] = df_train.iloc[start_index:end_index, 1:]
            boot_y[j*m:(j+1)*m, 0] = df_train.iloc[start_index:end_index, 0]

        # pre-test
        # estimate OLS on the block with all individual forecasts
        beta_hat = np.dot(
            np.linalg.inv(np.dot(np.transpose(boot_X), boot_X)),
            np.dot(np.transpose(boot_X), boot_y)
            )

        # residuals
        epsilon = boot_y - np.dot(boot_X, beta_hat)

        # compute the absolute t-statistics
        # compute S
        S_sum = np.full((K, K), 0, dtype=float)
        for e in range(p):
            for f in range(m):
                for g in range(m):

                    F_f = boot_X[e*m + f, :][:, np.newaxis]
                    F_g = boot_X[e*m + g, :][:, np.newaxis]
                    eps_f = epsilon[e*m + f]
                    eps_g = epsilon[e*m + g]
                    S_sum += np.dot(F_f * eps_f, np.transpose(F_g * eps_g))

        S = S_sum / (p*m)

        # compute H
        H_sum = np.full((K, K), 0, dtype=float)
        for e in range(p):
            for f in range(m):

                F_f = boot_X[e*m + f, :][:, np.newaxis]
                H_sum += np.dot(F_f, np.transpose(F_f))

        H = H_sum / (p*m)
        H_inv = np.linalg.inv(H)

        # variances
        beta_var = np.diag(
                1/np.sqrt(T) * np.linalg.multi_dot([H_inv, S, H_inv])
                )

        # near singularity may cause negative variance issues
        if (beta_var > 0).all():

            # t-statistics
            t_stats_abs = np.abs(beta_hat.flatten() / np.sqrt(beta_var))

            # variable selection
            boot_X_sel = boot_X[:, t_stats_abs > 1.96]

            # estimate OLS on the block with the selected forecasts
            boot_X_sel_t = np.transpose(boot_X_sel)
            gamma_hat = np.linalg.multi_dot(
                    [np.linalg.inv(np.dot(boot_X_sel_t, boot_X_sel)),
                     boot_X_sel_t, boot_y])

            # forecast out-of-sample
            pred_mat[:, i] = np.dot(
                    df_test.iloc[:, t_stats_abs > 1.96].values,
                    gamma_hat
                    ).flatten()

    # aggregation of forecasts
    pred = np.nanmean(pred_mat, axis=1)

    df_pred = pd.DataFrame(
            {"Bagging":  pred},
            index=df_test.index
            )

    return df_pred


def Componentwise_Boosting(df_train, df_test, nu):
    """
    The Componentwise Boosting method for gradual estimation
    of the linear forecast combination. The OLS fitting procedure is used
    as the base learner. The quadratic loss function is used. The parameter nu
    determines the amount of shrinkage and the number of boosting iterations
    is determined in the 5-fold cross-validation.

    """

    # number of individual forecasts and number of periods
    K = df_test.shape[1]
    T = df_train.shape[0]
    T_test = df_test.shape[0]

    # variable of interest
    y = df_train.iloc[:, 0].values[:, np.newaxis]
    y_bar = np.mean(y)

    # individual forecasts
    F = np.swapaxes(df_train.iloc[:, 1:].values, 0, 1)[:, :, np.newaxis]
    F_t = np.swapaxes(F, 1, 2)
    F_test = np.swapaxes(df_test.values, 0, 1)[:, :, np.newaxis]

    # 5-fold CV to determine optimal M
    # length of training and testing sets
    T_cv_test = int(T/5)
    T_cv_train = T - T_cv_test

    # initialize vector to store the precision of fit
    SSR_vec = np.full(1000, 0, dtype=float)

    # find the optimal number of boosting iterationts using 5-fold CV
    # CV folds
    for k in range(5):

        # definition of the test and training sample for the given CV round
        cv_index = np.full(T, True, dtype=bool)
        cv_index[(k*T_cv_test):((k+1)*T_cv_test)] = False

        y_cv = y[cv_index]
        y_cv_bar = np.mean(y_cv)
        y_cv_test = y[~cv_index]

        F_cv = F[:, cv_index, :]
        F_cv_t = F_t[:, :, cv_index]
        F_cv_test = F[:, ~cv_index, :]

        # initialization step
        psi = np.tile(y_cv_bar, (T_cv_train, 1))
        psi_test = np.tile(y_cv_bar, (T_cv_test, 1))

        # main steps
        for m in range(1000):

            # compute the negative gradient vector
            u = y_cv - psi

            # regress the negative gradient vector on the ind. forecasts
            beta_hat = np.matmul(
                    1/np.matmul(F_cv_t, F_cv),
                    np.matmul(F_cv_t, np.tile(u, (K, 1, 1)))
                    )

            # save the sums of the squared residuals
            SSR = np.dot(np.ones(T_cv_train), (u - (beta_hat * F_cv))**2)

            # find the minimum SSR and its corresponding ind.forecast
            k_star = np.argmin(SSR)

            # update
            psi += nu * beta_hat[k_star, :, :] * F_cv[k_star, :, :]
            psi_test += nu * beta_hat[k_star, :, :] * F_cv_test[
             k_star, :, :]

            # save the precision of the fit
            SSR_vec[m] += np.sum((y_cv_test - psi_test)**2)

    # find number of iterations for which the MSE is the lowest
    M = np.argmin(SSR_vec) + 1

    # final Gradient Boosting with pre-determined number of iterations M
    # initialization step
    psi = np.tile(y_bar, (T, 1))
    psi_test = np.tile(y_bar, (T_test, 1))

    # main steps
    for m in range(M):

        # compute the negative gradient vector
        u = y - psi

        # regress the negative gradient vector on the individual forecasts
        beta_hat = np.matmul(
                1/np.matmul(F_t, F),
                np.matmul(F_t, np.tile(u, (K, 1, 1)))
                )

        # save the sums of the squared residuals
        SSR = np.dot(np.ones(T), (u - (beta_hat * F))**2)

        # find the minimum SSR and its corresponding individual forecast
        k_star = np.argmin(SSR)

        # update
        psi += nu * beta_hat[k_star, :, :] * F[k_star, :, :]
        psi_test += nu * beta_hat[k_star, :, :] * F_test[k_star, :, :]

    # predictions
    df_pred = pd.DataFrame(
            {"Componentwise Boosting":  psi_test.flatten()},
            index=df_test.index
            )

    return df_pred


def AdaBoost(df_train, df_test, phi):
    """
    The AdaBoost algorithm, which combines the best metaparameters for
    time series forecasting (Barrow & Crone, 2016).

    """

    # number of periods
    T = df_train.shape[0]
    T_test = df_test.shape[0]

    # initialize observation weights
    w = np.full(T, 1, dtype=float)
    w_sum = np.sum(w)

    # matrix of individual forecasts
    F = df_train.iloc[:, 1:]

    # dependent variable
    y = df_train.iloc[:, 0]

    # prepare testing set
    test_F = df_test.values
    test_y_pred = np.full((T_test, 50), np.nan, dtype=float)

    # prepare MLP object
    MLP = MLPRegressor(hidden_layer_sizes=2, activation="tanh", solver="lbfgs")

    # AdaBoost iteration steps
    for i in range(50):

        # compute observation probabilities from weights
        prob = w / w_sum

        # generate the training sample
        train_index = np.random.choice(T, size=T, p=prob)
        train_F = df_train.iloc[train_index, 1:]
        # train_F_t = np.swapaxes(train_F, 0, 1)
        train_y = df_train.iloc[train_index, 0]

        # training the model by OLS
        # theta_hat = np.linalg.multi_dot(
        #        [np.linalg.inv(np.dot(train_F_t, train_F)), train_F_t,train_y]
        #         )

        # fit MLP
        MLP_fit = MLP.fit(train_F, train_y)

        # predictions (on the original dataset)
        # y_pred = np.dot(F, theta_hat)
        # test_y_pred[:, i] = np.dot(test_F, theta_hat)
        y_pred = MLP_fit.predict(F)
        test_y_pred[:, i] = MLP_fit.predict(test_F)

        # absolute relative errors
        ARE = np.abs((y - y_pred)/y)

        # threshold losses
        L = (ARE > phi)*1

        # weighted average loss
        L_bar = np.dot(L, prob)

        # model confidence
        beta = np.log(1/L_bar)

        # observation weights updating, absolute value for comp. reasons
        w = np.abs(np.multiply(w, beta**(1-L)))
        w_sum = np.sum(w)

    # forecast combination
    pred = np.mean(test_y_pred, axis=1)

    # output
    df_pred = pd.DataFrame(
            {"AdaBoost":  pred},
            index=df_test.index
            )

    return df_pred
# THE END OF MODULE
